<!--
   Licensed to the Apache Software Foundation (ASF) under one or more
   contributor license agreements.  See the NOTICE file distributed with
   this work for additional information regarding copyright ownership.
   The ASF licenses this file to You under the Apache License, Version 2.0
   (the "License"); you may not use this file except in compliance with
   the License.  You may obtain a copy of the License at
   http://www.apache.org/licenses/LICENSE-2.0
   Unless required by applicable law or agreed to in writing, software
   distributed under the License is distributed on an "AS IS" BASIS,
   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
   See the License for the specific language governing permissions and
   limitations under the License.
-->

# Experiment Implementations

## Overview

This document talks about implementation of experiment, flows and design considerations.

Experiment consists of following components, also interact with other Submarine or 3rd-party components, showing below: 

```


              +---------------------------------------+
 +----------+ |      Experiment Tasks                 |
 |Run       | |                                       |
 |Configs   | | +----------------------------------+  |
 +----------+ | |   Experiment Runnable Code       |  | +-----------------+
 +----------+ | |                                  |  | |Output Artifacts |
 |Input Data| | |     (Like train-job.py)          |  | |(Models, etc.)   |
 |          | | +----------------------------------+  | +-----------------+
 |          | | +----------------------------------+  |
 +----------+ | |   Experiment Deps (Like Python)  |  | +-------------+
              | +----------------------------------+  | |Logs/Metrics |
              | +----------------------------------+  | |             |
              | |  OS, Base Libaries (Like CUDA)   |  | +-------------+
              | +----------------------------------+  |
              +---------------------------------------+
                                 ^
                                 | (Launch Task with resources)
                                 +
                 +---------------------------------+
                 |Resource Manager (K8s/YARN/Cloud)|
                 +---------------------------------+
```

As showing in the above diagram, Submarine experiment consists of the following items: 

- On the left side, there're input data and run configs. 
- In the middle box, they're experiment tasks, it could be multiple tasks when we run distributed training, pipeline, etc. 
  - There're main runnable code, such as `train.py` for the training main entry point. 
  - The two boxes below: experiment depencies and OS/Base libraries we called `Submarine Environment Profile` or  `Environment` for short. Which defined what is the basic libraries to run the main experiment code. 
  - Experiment tasks are launched by Resource Manager, such as K8s/YARN/Cloud or just launched locally. There're resources constraints for each experiment tasks. (e.g. how much memory, cores, GPU, disk etc. can be used by tasks). 
- On the right side, they're artifacts generated by experiments: 
  - Output artifacts: Which are main output of the experiment, it could be model(s), or output data when we do batch prediction.
  - Logs/Metrics for further troubleshooting or understanding of experiment's quality.

For the rest of the design doc, we will talk about how we handle environment, code, and manage output/logs, etc.

## API of Experiment

This is not a full definition of experiment, for more details, please reference to experiment API. 

Here's just an example of experiment object which help developper to understand what included in an experiment.

```yaml
experiment:
       name: "abc",
       type: "script",
       environment: "team-default-ml-env"
       code:
   	       sync_mode: s3
           url: "s3://bucket/training-job.tar.gz" 
       parameter: > python training.py --iteration 10 
                    --input=s3://bucket/input output=s3://bucket/output
       resource_constraint: 
       	   res="mem=20gb, vcore=3, gpu=2"
       timeout: "30 mins"
```

This defined a "script" experiment, which has a name "abc", the name can be used to track the experiment. There's environment "team-default-ml-env" defined to make sure dependencies of the job can be downloaded properly before executing the job. 

`code` defined where the experiment code will be downloaded, we will support a couple of sync_mode like s3 (or abfs/hdfs), git, etc. 

Different types of experiments will have different specs, for example distributed Tensorflow spec may look like:

```yaml
experiment:
       name: "abc-distributed-tf",
       type: "distributed-tf",
       ps: 
            environment: "team-default-ml-cpu"
            resource_constraint: 
                 res="mem=20gb, vcore=3, gpu=0"
       worker: 
            environment: "team-default-ml-gpu"
            resource_constraint: 
                 res="mem=20gb, vcore=3, gpu=2"
       code:
   	       sync_mode: git
           url: "https://foo.com/training-job.git" 
       parameter: > python /code/training-job/training.py --iteration 10 
                    --input=s3://bucket/input output=s3://bucket/output
       tensorboard: enabled
       timeout: "30 mins"
```

Since we have different Docker image, one is using GPU and one is not using GPU, we can specify different environment and resource constraint.

## Manage environments for experiment

Please refer to [environment-implementation.md](./environment-implementation.md) for more details

## Manage storages for experiment

There're different types of storage, such as logs, metrics, dependencies (environments). For more details. Please refer to [storage-implementations](./storage-implementation.md) for more details. This also includes how to manage code for experiment code.

## Manage Pre-defined experiment libraries

## Flow: Submit an experiment

### Submit via SDK Flows.

To better understand experiment implementation, It will be good to understand what is the steps of experiment submission.

*Please note that below code is just pesudo code, not offical APIs.*

### Specify what environment to use

Before submit the environment, you have to choose what environment to choose. Environment defines dependencies, etc. of an experiment or a notebook. might looks like below:

```
conda_environment = 
"""
  name: conda-env
  channels:
    - defaults
  dependencies:
    - asn1crypto=1.3.0=py37_0
    - blas=1.0=mkl
    - ca-certificates=2020.1.1=0
    - certifi=2020.4.5.1=py37_0
    - cffi=1.14.0=py37hb5b8e2f_0
    - chardet=3.0.4=py37_1003
  prefix: /opt/anaconda3/envs/conda-env
"""

# This environment can be different from notebook's own environment
environment = create_environment {
    DockerImage = "ubuntu:16",
    CondaEnvironment = conda_environment
}
```

To better understand how environment works, please refer to [environment-implementation](./environment-implementation.md). 

### Create experiment, specify where's training code located, and parameters.

For  ad-hoc experiment (code located at S3), assume training code is part of the `training-job.tar.gz` and main class is `train.py`. When the job is launched, whatever specified in the localize_artifacts will be downloaded.

```
experiment = create_experiment {
    Environment = environment, 
    ExperimentConfig = {
       type = "adhoc",
       localize_artifacts = [
            "s3://bucket/training-job.tar.gz"
       ],
       name = "abc",
       parameter = "python training.py --iteration 10 --input="s3://bucket/input output="s3://bucket/output",
    }
}
experiment.run()
experiment.wait_for_finish(print_output=True)
```

##### Run notebook file in offline mode

It is possible we want to run a notebook file in offline mode, to do that, here's code to use to run a notebook code

```
experiment = create_experiment {
    Environment = environment, 
    ExperimentConfig = {
       type = "adhoc",
       localize_artifacts = [
            "s3://bucket/folder/notebook-123.ipynb"
       ],
       name = "abc",
       parameter = "runipy training.ipynb --iteration 10 --input="s3://bucket/input output="s3://bucket/output",
    }
}
experiment.run()
experiment.wait_for_finish(print_output=True)
```

##### Run pre-defined experiment library

```
experiment = create_experiment {
    # Here you can use default environment of library
    Environment = environment, 
    ExperimentConfig = {
       type = "template",
       name = "abc",
       # A unique name of template 
       template = "deepfm_ctr", 
       # yaml file defined what is the parameters need to be specified.
       parameter = {
           Input: "S3://.../input",
           Output: "S3://.../output"
           Training: {
              "batch_size": 512,
              "l2_reg": 0.01,
              ...
           }
       }
    }
}
experiment.run()
experiment.wait_for_finish(print_output=True)
```

## Summarize: Experiment v.s. Notebook session

There's a common misunderstanding about what is the differences between running experiment v.s. running task from a notebook session. We will talk about differences and commonalities:

**Differences**

|                                   | Experiment                                                   | Notebook Session                                             |
| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| Run mode                          | Offline                                                      | Interactive                                                  |
| Output Artifacts (a.k.a model)    | Persisted in a shared storage (like S3/NFS)                  | Local in the notebook session container, could be emphameral |
| Run history (meta, logs, metrics) | Meta/logs/metrics can be traced from experiment UI (or corresponding API) | No run history can be traced from Submarine UI/API. Can view the current running paragraph's log/metrics, etc. |
| What to run?                      | Code from Docker image or shared storage (like Tarball on S3, Github, etc.) | Local in the notebook's paragraph                            |

**Commonalities** 

|             | Experiment & Notebook Session                     |
| ----------- | ------------------------------------------------- |
| Environment | They can share the same Environment configuration |

**Other Common Myths:** 

1) Can we specify different environment of experiment when submit from 